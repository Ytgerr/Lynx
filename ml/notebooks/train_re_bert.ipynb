{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Relation Extraction with Russian BERT\n",
                "\n",
                "This notebook fine-tunes a BERT model (`DeepPavlov/rubert-base-cased`) for Relation Extraction (RE) on the provided dataset.\n",
                "\n",
                "## Approach\n",
                "We use the **Entity Marker** approach:\n",
                "1. For each pair of entities in a sentence, we mark them with special tokens `<e1>`, `</e1>`, `<e2>`, `</e2>`.\n",
                "2. The model classifies the relation between the marked entities.\n",
                "3. If no relation exists in the dataset for a pair, we assign the label `NO_RELATION`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install transformers datasets torch scikit-learn tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import torch\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
                "from torch.optim import AdamW\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import classification_report\n",
                "from tqdm.auto import tqdm\n",
                "import os\n",
                "\n",
                "# Configuration\n",
                "DATASET_PATH = r\"..\\datasets\\process\\RE_dataset\"\n",
                "MODEL_NAME = \"DeepPavlov/rubert-base-cased\"\n",
                "MAX_LEN = 128\n",
                "BATCH_SIZE = 16\n",
                "EPOCHS = 5\n",
                "LEARNING_RATE = 2e-5\n",
                "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {DEVICE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load and Process Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_data(file_path):\n",
                "    data = []\n",
                "    with open(file_path, 'r', encoding='utf-8') as f:\n",
                "        for line in f:\n",
                "            line = line.strip()\n",
                "            if not line:\n",
                "                continue\n",
                "            try:\n",
                "                data.append(json.loads(line))\n",
                "            except json.JSONDecodeError:\n",
                "                print(f\"Skipping invalid line: {line[:50]}...\")\n",
                "    return data\n",
                "\n",
                "raw_data = load_data(DATASET_PATH)\n",
                "print(f\"Loaded {len(raw_data)} sentences.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def process_examples(data):\n",
                "    examples = []\n",
                "    relation_types = set()\n",
                "    \n",
                "    for entry in data:\n",
                "        tokens = entry['tokens']\n",
                "        entities = entry['entities']\n",
                "        relations = entry.get('relations', [])\n",
                "        \n",
                "        # Map (head_start_idx, tail_start_idx) -> relation_label\n",
                "        rel_map = {}\n",
                "        for rel in relations:\n",
                "            # rel is [head_token_idx, tail_token_idx, label]\n",
                "            head_idx, tail_idx, label = rel\n",
                "            rel_map[(head_idx, tail_idx)] = label\n",
                "            relation_types.add(label)\n",
                "            \n",
                "        # Generate all pairs of entities\n",
                "        for i, e1 in enumerate(entities):\n",
                "            for j, e2 in enumerate(entities):\n",
                "                if i == j:\n",
                "                    continue\n",
                "                \n",
                "                e1_start, e1_end, e1_type = e1\n",
                "                e2_start, e2_end, e2_type = e2\n",
                "                \n",
                "                # Check if relation exists\n",
                "                # The dataset uses start token index for relation mapping\n",
                "                label = rel_map.get((e1_start, e2_start), \"NO_RELATION\")\n",
                "                if label == \"NO_RELATION\":\n",
                "                    # Optional: Downsample NO_RELATION if too many\n",
                "                    pass\n",
                "                \n",
                "                examples.append({\n",
                "                    'tokens': tokens,\n",
                "                    'e1_span': (e1_start, e1_end),\n",
                "                    'e2_span': (e2_start, e2_end),\n",
                "                    'label': label\n",
                "                })\n",
                "                \n",
                "    return examples, sorted(list(relation_types))\n",
                "\n",
                "examples, relation_labels = process_examples(raw_data)\n",
                "if \"NO_RELATION\" not in relation_labels:\n",
                "    relation_labels.append(\"NO_RELATION\")\n",
                "    \n",
                "label2id = {l: i for i, l in enumerate(relation_labels)}\n",
                "id2label = {i: l for l, i in label2id.items()}\n",
                "\n",
                "print(f\"Generated {len(examples)} examples.\")\n",
                "print(f\"Relation types: {relation_labels}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Tokenization and Dataset Class\n",
                "We insert special markers `<e1>`, `</e1>`, `<e2>`, `</e2>` around the subject and object entities."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "\n",
                "# Add special tokens for entity markers\n",
                "special_tokens = {'additional_special_tokens': ['<e1>', '</e1>', '<e2>', '</e2>']}\n",
                "tokenizer.add_special_tokens(special_tokens)\n",
                "\n",
                "class REDataset(Dataset):\n",
                "    def __init__(self, examples, tokenizer, label2id, max_len=128):\n",
                "        self.examples = examples\n",
                "        self.tokenizer = tokenizer\n",
                "        self.label2id = label2id\n",
                "        self.max_len = max_len\n",
                "        \n",
                "    def __len__(self):\n",
                "        return len(self.examples)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        ex = self.examples[idx]\n",
                "        tokens = ex['tokens']\n",
                "        e1_start, e1_end = ex['e1_span']\n",
                "        e2_start, e2_end = ex['e2_span']\n",
                "        \n",
                "        # Construct sentence with markers\n",
                "        # We need to be careful with indices as we insert tokens\n",
                "        # Strategy: Reconstruct the list of tokens with markers inserted\n",
                "        \n",
                "        new_tokens = []\n",
                "        for i, token in enumerate(tokens):\n",
                "            if i == e1_start:\n",
                "                new_tokens.append('<e1>')\n",
                "            if i == e2_start:\n",
                "                new_tokens.append('<e2>')\n",
                "            \n",
                "            new_tokens.append(token)\n",
                "            \n",
                "            if i == e1_end:\n",
                "                new_tokens.append('</e1>')\n",
                "            if i == e2_end:\n",
                "                new_tokens.append('</e2>')\n",
                "                \n",
                "        text = \" \".join(new_tokens)\n",
                "        \n",
                "        encoding = self.tokenizer(\n",
                "            text,\n",
                "            max_length=self.max_len,\n",
                "            padding='max_length',\n",
                "            truncation=True,\n",
                "            return_tensors='pt'\n",
                "        )\n",
                "        \n",
                "        return {\n",
                "            'input_ids': encoding['input_ids'].flatten(),\n",
                "            'attention_mask': encoding['attention_mask'].flatten(),\n",
                "            'labels': torch.tensor(self.label2id[ex['label']], dtype=torch.long)\n",
                "        }\n",
                "\n",
                "train_ex, val_ex = train_test_split(examples, test_size=0.2, random_state=42)\n",
                "\n",
                "train_dataset = REDataset(train_ex, tokenizer, label2id, MAX_LEN)\n",
                "val_dataset = REDataset(val_ex, tokenizer, label2id, MAX_LEN)\n",
                "\n",
                "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
                "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Model Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = AutoModelForSequenceClassification.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    num_labels=len(label2id)\n",
                ")\n",
                "# Resize embeddings because we added special tokens\n",
                "model.resize_token_embeddings(len(tokenizer))\n",
                "model.to(DEVICE)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
                "total_steps = len(train_loader) * EPOCHS\n",
                "scheduler = get_linear_schedule_with_warmup(\n",
                "    optimizer, \n",
                "    num_warmup_steps=0, \n",
                "    num_training_steps=total_steps\n",
                ")\n",
                "\n",
                "def train_epoch(model, data_loader, optimizer, scheduler, device):\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    \n",
                "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
                "        input_ids = batch['input_ids'].to(device)\n",
                "        attention_mask = batch['attention_mask'].to(device)\n",
                "        labels = batch['labels'].to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        outputs = model(\n",
                "            input_ids=input_ids,\n",
                "            attention_mask=attention_mask,\n",
                "            labels=labels\n",
                "        )\n",
                "        \n",
                "        loss = outputs.loss\n",
                "        total_loss += loss.item()\n",
                "        \n",
                "        loss.backward()\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
                "        optimizer.step()\n",
                "        scheduler.step()\n",
                "        \n",
                "    return total_loss / len(data_loader)\n",
                "\n",
                "def eval_model(model, data_loader, device):\n",
                "    model.eval()\n",
                "    preds = []\n",
                "    true_labels = []\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
                "            input_ids = batch['input_ids'].to(device)\n",
                "            attention_mask = batch['attention_mask'].to(device)\n",
                "            labels = batch['labels'].to(device)\n",
                "            \n",
                "            outputs = model(\n",
                "                input_ids=input_ids,\n",
                "                attention_mask=attention_mask\n",
                "            )\n",
                "            \n",
                "            _, predicted = torch.max(outputs.logits, dim=1)\n",
                "            preds.extend(predicted.cpu().numpy())\n",
                "            true_labels.extend(labels.cpu().numpy())\n",
                "            \n",
                "    return true_labels, preds"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for epoch in range(EPOCHS):\n",
                "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
                "    train_loss = train_epoch(model, train_loader, optimizer, scheduler, DEVICE)\n",
                "    print(f\"Train loss: {train_loss:.4f}\")\n",
                "    \n",
                "    true_labels, preds = eval_model(model, val_loader, DEVICE)\n",
                "    print(classification_report(true_labels, preds, labels=list(label2id.values()), target_names=list(label2id.keys())))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Save Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "output_dir = \"./re_bert_model\"\n",
                "if not os.path.exists(output_dir):\n",
                "    os.makedirs(output_dir)\n",
                "\n",
                "model.save_pretrained(output_dir)\n",
                "tokenizer.save_pretrained(output_dir)\n",
                "print(f\"Model saved to {output_dir}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}