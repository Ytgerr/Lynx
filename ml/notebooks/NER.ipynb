{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a13c6b0",
   "metadata": {},
   "source": [
    "### NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdd0c794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\k4ty2\\pythonVsCode\\Lynx\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\k4ty2\\pythonVsCode\\Lynx\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\k4ty2\\.cache\\huggingface\\hub\\models--dslim--bert-base-NER. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-PER', 'score': np.float32(0.9990139), 'index': 4, 'word': 'Wolfgang', 'start': 11, 'end': 19}, {'entity': 'B-LOC', 'score': np.float32(0.999645), 'index': 9, 'word': 'Berlin', 'start': 34, 'end': 40}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd5c29b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-PER', 'score': np.float32(0.9990139), 'index': 4, 'word': 'Wolfgang', 'start': 11, 'end': 19}, {'entity': 'B-LOC', 'score': np.float32(0.999645), 'index': 9, 'word': 'Berlin', 'start': 34, 'end': 40}]\n"
     ]
    }
   ],
   "source": [
    "example = \"My name is Wolfgang and I live in Berlin\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95666616",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b37c64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T11:15:14.249432Z",
     "iopub.status.busy": "2025-10-06T11:15:14.249080Z",
     "iopub.status.idle": "2025-10-06T11:15:14.261679Z",
     "shell.execute_reply": "2025-10-06T11:15:14.260935Z",
     "shell.execute_reply.started": "2025-10-06T11:15:14.249390Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\k4ty2\\pythonVsCode\\Lynx\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d15192",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063fd6a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T11:29:53.070100Z",
     "iopub.status.busy": "2025-10-06T11:29:53.069828Z",
     "iopub.status.idle": "2025-10-06T11:29:57.506173Z",
     "shell.execute_reply": "2025-10-06T11:29:57.505600Z",
     "shell.execute_reply.started": "2025-10-06T11:29:53.070080Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['gold_label', 'gold_token', 'doc_idx', 'sent_idx'],\n",
      "        num_rows: 80531\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['gold_label', 'gold_token', 'doc_idx', 'sent_idx'],\n",
      "        num_rows: 10233\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['gold_label', 'gold_token', 'doc_idx', 'sent_idx'],\n",
      "        num_rows: 25957\n",
      "    })\n",
      "})\n",
      "{'gold_label': 0, 'gold_token': 'Kenyan', 'doc_idx': 0, 'sent_idx': 0}\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 3262\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 402\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 1075\n",
      "    })\n",
      "})\n",
      "{'tokens': ['Kenyan', 'Firms', 'Eye', 'Deals', 'During', 'Obama', 'Summit', 'Tagged', ':', 'The', 'Global', 'Entrepreneurship', 'Summit', ',', 'launched', 'by', 'President', 'Obama', 'in', '2009', ',', 'brings', 'together', 'entrepreneurs', 'and', 'investors', 'from', 'across', 'Africa', 'and', 'around', 'the', 'world', 'annually', 'to', 'showcase', 'innovative', 'projects', ',', 'exchange', 'new', 'ideas', ',', 'and', 'help', 'spur', 'economic', 'opportunity', '.'], 'ner_tags': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": \"/kaggle/input/finer-ord/train.csv\",\n",
    "        \"validation\": \"/kaggle/input/finer-ord/val.csv\",\n",
    "        \"test\": \"/kaggle/input/finer-ord/test.csv\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][0])\n",
    "\n",
    "\n",
    "def group_sentences(ds_split):\n",
    "    grouped = {}\n",
    "    for ex in ds_split:\n",
    "        token = ex['gold_token']\n",
    "        if token is None or not isinstance(token, str) or not token.strip():  \n",
    "            continue\n",
    "        key = (ex['doc_idx'], ex['sent_idx'])\n",
    "        if key not in grouped:\n",
    "            grouped[key] = {'tokens': [], 'ner_tags': []}\n",
    "        grouped[key]['tokens'].append(token)\n",
    "        grouped[key]['ner_tags'].append(ex['gold_label'])\n",
    "    \n",
    "    # Only include non-empty sentences\n",
    "    data = [{'tokens': val['tokens'], 'ner_tags': val['ner_tags']} for key, val in grouped.items() if val['tokens']]\n",
    "    return Dataset.from_list(data)\n",
    "\n",
    "dataset['train'] = group_sentences(dataset['train'])\n",
    "dataset['validation'] = group_sentences(dataset['validation'])\n",
    "dataset['test'] = group_sentences(dataset['test'])\n",
    "\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f96208",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7bc893d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T11:36:49.475097Z",
     "iopub.status.busy": "2025-10-06T11:36:49.474823Z",
     "iopub.status.idle": "2025-10-06T11:36:50.598004Z",
     "shell.execute_reply": "2025-10-06T11:36:50.597223Z",
     "shell.execute_reply.started": "2025-10-06T11:36:49.475077Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['O', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0756b3cdc8c44b8abde4f1da9613bf95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3262 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f748293c7774051bb4f665aed09a2ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/402 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272920783f18430c848331055a0853e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1075 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "label_to_id = {\n",
    "    'O': 0,\n",
    "    'B-PER': 1,\n",
    "    'I-PER': 2,\n",
    "    'B-LOC': 3,\n",
    "    'I-LOC': 4,\n",
    "    'B-ORG': 5,\n",
    "    'I-ORG': 6\n",
    "}\n",
    "id_to_label = {v: k for k, v in label_to_id.items()}\n",
    "label_list = list(label_to_id.keys())\n",
    "\n",
    "print(\"Labels:\", label_list)\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    for i, sent in enumerate(examples[\"tokens\"]):\n",
    "        for j, tok in enumerate(sent):\n",
    "            if not isinstance(tok, str):\n",
    "                print(f\"Non-string token in example {i}, position {j}: {tok}\")\n",
    "    \n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], \n",
    "        truncation=True,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    all_labels = []\n",
    "    for i, label_seq in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_seq[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        all_labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30af0ef4",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75ce501e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T11:36:54.440504Z",
     "iopub.status.busy": "2025-10-06T11:36:54.440003Z",
     "iopub.status.idle": "2025-10-06T11:36:54.745372Z",
     "shell.execute_reply": "2025-10-06T11:36:54.744649Z",
     "shell.execute_reply.started": "2025-10-06T11:36:54.440479Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dslim/bert-base-NER and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([7]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([9, 768]) in the checkpoint and torch.Size([7, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=len(label_list), \n",
    "    id2label=id_to_label, \n",
    "    label2id=label_to_id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96738bf1",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0abaa8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T11:36:56.643564Z",
     "iopub.status.busy": "2025-10-06T11:36:56.643063Z",
     "iopub.status.idle": "2025-10-06T11:38:26.229490Z",
     "shell.execute_reply": "2025-10-06T11:38:26.228865Z",
     "shell.execute_reply.started": "2025-10-06T11:36:56.643537Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36/3132501564.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начинаем обучение...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='612' max='612' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [612/612 01:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.047500</td>\n",
       "      <td>0.044900</td>\n",
       "      <td>0.883607</td>\n",
       "      <td>0.885057</td>\n",
       "      <td>0.884331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.022600</td>\n",
       "      <td>0.041599</td>\n",
       "      <td>0.912252</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.908491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.017600</td>\n",
       "      <td>0.042218</td>\n",
       "      <td>0.907743</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.906250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение завершено.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    disable_tqdm=False,\n",
    "    report_to=\"none\"  # если хочешь отключить wandb или другие логгеры\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [[id_to_label[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [id_to_label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, true_predictions),\n",
    "        \"recall\": recall_score(true_labels, true_predictions),\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "    }\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Начинаем обучение...\")\n",
    "trainer.train()\n",
    "print(\"Обучение завершено.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b00f12",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "def20a9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T11:39:12.154881Z",
     "iopub.status.busy": "2025-10-06T11:39:12.154593Z",
     "iopub.status.idle": "2025-10-06T11:39:15.305621Z",
     "shell.execute_reply": "2025-10-06T11:39:15.304886Z",
     "shell.execute_reply.started": "2025-10-06T11:39:12.154858Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [68/68 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06925616413354874, 'eval_precision': 0.7867892976588629, 'eval_recall': 0.8261633011413521, 'eval_f1': 0.8059957173447537, 'eval_runtime': 2.3791, 'eval_samples_per_second': 451.856, 'eval_steps_per_second': 28.583, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/kaggle/working/models/ner_model/tokenizer_config.json',\n",
       " '/kaggle/working/models/ner_model/special_tokens_map.json',\n",
       " '/kaggle/working/models/ner_model/vocab.txt',\n",
       " '/kaggle/working/models/ner_model/added_tokens.json',\n",
       " '/kaggle/working/models/ner_model/tokenizer.json')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "print(results)\n",
    "\n",
    "trainer.save_model(\"/kaggle/working/models/ner_model\")\n",
    "tokenizer.save_pretrained(\"/kaggle/working/models/ner_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e42451e6-837b-4a13-806d-c15859bbc6bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T11:41:33.153698Z",
     "iopub.status.busy": "2025-10-06T11:41:33.153428Z",
     "iopub.status.idle": "2025-10-06T11:41:33.661956Z",
     "shell.execute_reply": "2025-10-06T11:41:33.661312Z",
     "shell.execute_reply.started": "2025-10-06T11:41:33.153678Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Barack Obama, Label: PER, Score: 0.9935\n",
      "Entity: Hawaii, Label: LOC, Score: 0.9948\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "model_path = \"/kaggle/working/models/ner_model\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "text = \"Barack Obama was born in Hawaii.\"\n",
    "\n",
    "ner_results = ner_pipeline(text)\n",
    "\n",
    "for entity in ner_results:\n",
    "    print(f\"Entity: {entity['word']}, Label: {entity['entity_group']}, Score: {entity['score']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8415256,
     "sourceId": 13278679,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8415261,
     "sourceId": 13278685,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
